---
title: "Prediction of Tornado Occurance in the Twin Cities"
subtitle: "INFO 523 - Final Project"
author: 
  - name: "The Crengineers - Tyler Hart"
    affiliations:
      - name: "College of Information Science, College of Systems and Industrial Engineering, University of Arizona"
description: "TODO: An analysis of historical tornado patterns in the area of Twin Ciries from 2020-Current(2025) to predict Tornado Occurence"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

TODO: Add project abstract here.

## Intro

The primary goal of this project is to use publically available daily weather data to form relationships with NOAA records of tornado formation. This is crucial because unlike hurricanes which have a longer warning timeline, tornadoes are often learned about shortly before they occur. This can cause more extreme damage as people do not have time to secure crucial belongins or themselves in a non-affected area. Daily weather forecast are available and usually a part of every person's daily life. Whether that be through a weather app or the news this publically available data could be potentially utilized to help warn/inform people of the chance of tornado occurring in their areas. One approach to this problem is to create regression models such as, Lasso, Ridge, and Random Forest using polynomial features. TODO: More Here

This project is special to me as I have had the unfortunate experience of having friends and families lives be altered by unexpected tornados. Because of this I am going to focus my analysis to the Twin Cities area, pictured in the Map of Analysis Area Section. This means I will focus my analysis to the surrounding counties and weather data from the Mineapolis-Saint Paul, MSP, airport.  This has lead me to two try and answer the two questions seen below in the Questions section. 

# Questions: 

1. Can I develop a model that successfully predicts which days tornado formation will occur using daily weather data for the Twin Cities area? 

2. Can I create a GUI/dashboard to quickly allow users to tune/view predictions. 

# Map of Analysis Area: 

The surrounding colonies to the Twin Cities, also known as MSP, are the only region where the data is valid. Since the airport is located in the heart of the twin cities this analysis will only encompass the counties shown in the map below:
![County Map](images/MN-counties.png "Picture of Applicable Counties")
The counties were selected on a region selection basis. To properly select the region where the data is valid I drew a 60 mile circle around the MSP airport and selected any counties within that region. 60 miles was chosen to give me both ample data but also only have roughly an hour drive from the airport as I felt that was an applicable size. 

# Section Definitions:

In this write-up, there will be 6 main sections, Intro, Dataset and Feature Engineering, Exploratory Data Analysis, Model Creation and Tuning, Results and Analysis, and App/Dashboard. In the Intro section I have identified the purpose, scope and major questions I intend to try and answer. In the Dataset and Feature Engineering section I will discuss the cleaned up dataset with the engineered features pertinent to the future model creation and analysis. In the Ecploratory Data Analysis Section, I will show plots pertaining to the variable of interest as well as comment on different relationships I noticed in a primary analysis. In the Model Creation and Tuning section I will discuss why I chose each model and the corresponding cross validation I used to validate/make the most representative model possible. In the Results and Analysis section, I will show analysis results and plots as well as discuss the results of the model tuning and cross validation. Laslty I will attempt to collate those results into a dashboard for user interfacing and quick use to inform of tornado creation. 

## DataSet and Feature Engineering

# DataSet Description
I have 2 raw datasets that I have gathered in trying to answer these questions. The 1st data set found in this repo at [NOAA Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/storm_data_search_results.csv)* is the historical data of every tornado that has been reported  as far back as 1952. This data contains the relevant county, data, and tornado information, to allow me to filter and clean the data to compare to the 2nd data set I will be using. The 2nd data set found in this repo at [API Daily Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/mn_weather_data.csv)* is the historical daily weather data measured at MSP International Airport as far back as 2000. This data contains daily weather information that I will be training my regression models against to hopefully predict tornado formation. The combination** of these datasets can be found in this repo at [Combined Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/tornado_days.csv)*. The combined dataset is 

# Feature Engineering
This data will need a few more columns. Namely I will need to Feature Engineer a column for tornado_occurred. This column will simply be boolean represented as an integer that is true if a tornado occurred on that day in an applicable county. Another column I will need TODO: More Here

* for more information on what is contained in the dataset plese go to [Data Dictionary](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/README.md)
The dataset I am going to be using is a combination of the NOAA tornado archives data and the daily weather open-meteo API data. 

** for more detailed information about the merge process look in my proposal at [Proposal Link](https://github.com/INFO-523-SU25/final-project-hart/blob/main/proposal.qmd)

```{python}

InitialDS = pd.read_csv('data/tornado_days.csv')

```

## Exploratory Data Analysis (EDA)

```{python}
def UnivariateAnalysis(df, column = None):
    """
    Perform univariate analysis on the dataset.
    This function displays descriptive statistics 
    and creates the appropriate plot for both 
    numerical and categorical variables.
    """
    if column != None:
        numeric_columns = column
    else:
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()  # Get numeric columns for plotting
    # print(f"Numeric columns in {df} dataset:", numeric_columns)  # Print the list of numeric
    for data in numeric_columns:
        print(df[data].describe())  # Display descriptive statistics for each numeric column
        sns.histplot(data=df, x=data, bins=30).set_title(f'Distribution of {data}')
        plt.show()  # Display the histogram for each numeric column

    if column != None:
        categorical_columns = column
    else:
        categorical_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()  # Get numeric columns for plotting
    # print(f"Categoric columns in {df} dataset:", categorical_columns)  # Print the list of categoric
    for data in categorical_columns:
        print(df[data].describe())  # Display descriptive statistics for each categoric column
        sns.countplot(data=df, x=data).set_title(f'Count of {data}')
        plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
        plt.tight_layout()  # Adjust layout to prevent label overlap
        plt.show()  # Display the count plot for each categoric column

#Down selected to a few interesting columns
sns.pairplot(data=InitialDS, x_vars=[], y_vars=[])

# '''
# Perform univariate analysis on the datasets.
# '''
UnivariateAnalysis(InitialDS)
```
TODO: Add EDA here.

## Model Creation and Tuning

# Regression Model 1
- **Model 1**
  - 
TODO: Add Model Creation here.
TODO: Add Model Tuning here.

```{python}

```
## Results and Analysis
```{python}

```
TODO: Add Results here.
TODO: Add Analysis here.

## App/Dashboard
```{python}

```
TODO: Add App/Dashboard here.

