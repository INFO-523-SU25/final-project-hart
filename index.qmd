---
title: "Prediction of Tornado Occurance in the Twin Cities"
subtitle: "INFO 523 - Final Project"
author: 
  - name: "The Crengineers - Tyler Hart"
    affiliations:
      - name: "College of Information Science, College of Systems and Industrial Engineering, University of Arizona"
description: "TODO: An analysis of historical tornado patterns in the area of Twin Ciries from 2020-Current(2025) to predict Tornado Occurence"
format:
   html:
    code-tools: true
    code-overflow: wrap
    embed-resources: true
editor: visual
execute:
  warning: false
  echo: false
jupyter: python3
---

## Abstract

TODO: Add project abstract here.

## Intro

The primary goal of this project is to use publically available daily weather data to form relationships with NOAA records of tornado formation. This is crucial because unlike hurricanes which have a longer warning timeline, tornadoes are often learned about shortly before they occur. This can cause more extreme damage as people do not have time to secure crucial belongins or themselves in a non-affected area. Daily weather forecast are available and usually a part of every person's daily life. Whether that be through a weather app or the news this publically available data could be potentially utilized to help warn/inform people of the chance of tornado occurring in their areas. One approach to this problem is to create regression models such as, Lasso, Ridge, and Random Forest using polynomial features. TODO: More Here

This project is special to me as I have had the unfortunate experience of having friends and families lives be altered by unexpected tornados. Because of this I am going to focus my analysis to the Twin Cities area, pictured in the Map of Analysis Area Section. This means I will focus my analysis to the surrounding counties and weather data from the Mineapolis-Saint Paul, MSP, airport.  This has lead me to two try and answer the two questions seen below in the Questions section. 

# Questions: 

1. Can I develop a model that successfully classifies which days tornado formation will occur using daily weather data for the Twin Cities area? 

2. Can I create a GUI/dashboard to quickly allow users to tune/view predictions. 

# Map of Analysis Area: 

The surrounding colonies to the Twin Cities, also known as MSP, are the only region where the data is valid. Since the airport is located in the heart of the twin cities this analysis will only encompass the counties shown in the map below:

![County Map](images/MN-counties.png "Picture of Applicable Counties")

The counties were selected on a region selection basis. To properly select the region where the data is valid I drew a 60 mile circle around the MSP airport and selected any counties within that region. 60 miles was chosen to give me both ample data but also only have roughly an hour drive from the airport as I felt that was an applicable size. 

# Section Definitions:

In this write-up, there will be 6 main sections, Intro, Dataset and Feature Engineering, Exploratory Data Analysis, Model Creation and Tuning, Results and Analysis, and App/Dashboard. In the Intro section I have identified the purpose, scope and major questions I intend to try and answer. In the Dataset and Feature Engineering section I will discuss the cleaned up dataset with the engineered features pertinent to the future model creation and analysis. In the Ecploratory Data Analysis Section, I will show plots pertaining to the variable of interest as well as comment on different relationships I noticed in a primary analysis. In the Model Creation and Tuning section I will discuss why I chose each model and the corresponding cross validation I used to validate/make the most representative model possible. In the Results and Analysis section, I will show analysis results and plots as well as discuss the results of the model tuning and cross validation. Laslty I will attempt to collate those results into a dashboard for user interfacing and quick use to inform of tornado creation. 

## DataSet and Feature Engineering

# DataSet Description
I have 2 raw datasets that I have gathered in trying to answer these questions. The 1st data set found in this repo at [NOAA Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/storm_data_search_results.csv)* is the historical data of every tornado that has been reported  as far back as 1952. This data contains the relevant county, data, and tornado information, to allow me to filter and clean the data to compare to the 2nd data set I will be using. The 2nd data set found in this repo at [API Daily Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/mn_weather_data.csv)* is the historical daily weather data measured at MSP International Airport as far back as 2000. This data contains daily weather information that I will be training my regression models against to hopefully predict tornado formation. The combination** of these datasets can be found in this repo at [Combined Data](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/tornado_days.csv)*. The combined dataset is the combination of these two datasets with new features from the Feature Engineering section. 

# Feature Engineering
This data will need a few more columns. Namely I will need to Feature Engineer a column for tornado_occurred. This column will simply be boolean represented as an integer that is true if a tornado occurred on that day in an applicable county. Another column I will need time_of_year. This column will represent the month the event occurred in. This is imoortant as tornadoes are seasonal in most of the United States. This will help find relationships and be a potential trainer of the model. 

| Variable | Type | Description | Unit |
|----------|------|-------------|------|
| tornado_occurred | Bool (Float) | Did a tornado occurr on this day (1 if yes, 0 if no) | None |
| time_of_year | Int | The index of the month the event occurred | Month | 

\* for more information on what is contained in the dataset plese go to [Data Dictionary](https://github.com/INFO-523-SU25/final-project-hart/blob/main/data/README.md)
The dataset I am going to be using is a combination of the NOAA tornado archives data and the daily weather open-meteo API data. 

** for more detailed information about the merge process look in my proposal at [Proposal Link](https://github.com/INFO-523-SU25/final-project-hart/blob/main/proposal.qmd)
```{python}
#| label: load-pkgs
#| message: false
# Here are the packages I need as of now to do basic dataset operations
import numpy as np
import statsmodels.api as sm
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
# from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV,cross_val_score
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report,roc_curve, roc_auc_score, accuracy_score
from sklearn.decomposition import PCA
# import tkinter as tk
# from tkinter import ttk

```

```{python}
#| label: clean-data
#| message: false

AllDaysWithTornados = pd.read_csv('data/tornado_days.csv')
AllDaysWithTornados = AllDaysWithTornados.drop('Unnamed: 0', axis=1)
AllDaysWithTornados['CZ_NAME_STR'] = AllDaysWithTornados['CZ_NAME_STR'].fillna('None')
AllDaysWithTornados['TOR_F_SCALE'] = AllDaysWithTornados['TOR_F_SCALE'].fillna('None')

label_encoder = LabelEncoder()
AllDaysWithTornados['CZ_NAME_STR'] = label_encoder.fit_transform(AllDaysWithTornados['CZ_NAME_STR'])
# print("Encoded Labels:", AllDaysWithTornados['CZ_NAME_STR'])
AllDaysWithTornados['TOR_F_SCALE'] = label_encoder.fit_transform(AllDaysWithTornados['TOR_F_SCALE'])
# print("Encoded Labels:", AllDaysWithTornados['TOR_F_SCALE'])
# AllDaysWithTornados['TOR_F_SCALE'].unique()
# AllDaysWithTornados['CZ_NAME_STR'].unique()

```

## Exploratory Data Analysis (EDA)

```{python}
#| label: eda
#| message: false
def UnivariateAnalysis(df, column = None):
    """
    Perform univariate analysis on the dataset.
    This function displays descriptive statistics 
    and creates the appropriate plot for both 
    numerical and categorical variables.
    """
    if column != None:
        numeric_columns = column
    else:
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()  # Get numeric columns for plotting
    # print(f"Numeric columns in {df} dataset:", numeric_columns)  # Print the list of numeric
    for data in numeric_columns:
        print(df[data].describe())  # Display descriptive statistics for each numeric column
        sns.histplot(data=df, x=data, bins=30).set_title(f'Distribution of {data}')
        plt.show()  # Display the histogram for each numeric column

    if column != None:
        categorical_columns = column
    else:
        categorical_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()  # Get numeric columns for plotting
    # print(f"Categoric columns in {df} dataset:", categorical_columns)  # Print the list of categoric
    for data in categorical_columns:
        print(df[data].describe())  # Display descriptive statistics for each categoric column
        sns.countplot(data=df, x=data).set_title(f'Count of {data}')
        plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
        plt.tight_layout()  # Adjust layout to prevent label overlap
        plt.show()  # Display the count plot for each categoric column

#Down selected to a few interesting columns
sns.pairplot(data=AllDaysWithTornados, x_vars=['temperature_2m_mean', 'precipitation_sum', 'dew_point_2m_mean', 'wind_direction_10m_dominant', 'surface_pressure_mean', 'wind_gusts_10m_max', 'wind_speed_10m_max', 'date', 'relative_humidity_2m_mean', 'time_of_year', 'weather_code'], y_vars=['tornado_occurred'])
plt.show()
sns.pairplot(data=AllDaysWithTornados, x_vars=['temperature_2m_mean', 'precipitation_sum', 'dew_point_2m_mean', 'wind_direction_10m_dominant', 'surface_pressure_mean', 'wind_gusts_10m_max', 'wind_speed_10m_max', 'relative_humidity_2m_mean', 'tornado_occurred', 'time_of_year', 'weather_code'], y_vars=['date'])
plt.show()
sns.pairplot(data=AllDaysWithTornados, x_vars=['temperature_2m_mean', 'precipitation_sum', 'dew_point_2m_mean', 'wind_direction_10m_dominant', 'surface_pressure_mean', 'wind_gusts_10m_max', 'wind_speed_10m_max', 'relative_humidity_2m_mean', 'tornado_occurred', 'date', 'weather_code'], y_vars=['time_of_year'])
plt.show()

# '''
# Perform univariate analysis on the datasets.
# '''
# UnivariateAnalysis(InitialDS)
```

The first thing I notice is that there looks like clear conditions for what makes tornados. Mean temperature is roughly 60 to 80 degrees, dew point is 50 to 75, wind direction over 10 minutes is generaly 100 to 200 degrees, surface pressure is generally grouped, wind gust and wind spee maximums are generally 20 to 40 mph and 10 to 20 mph respectively. 

Trends when compared to date look fairly normally distributed from year to year. It looks like a dense cloud for most of the variables which tells me conditions have not changed over time. This gives me hope that the trends are consistent and will continue going forward. I plan on using this information to say trends are consistent and can be extended into the future forecasting.

Since tornados are seasonal in most of the US I wanted to also see if the initial trends I noticed were correlated. On an initial look the conditions do appear to be connected as the middle months (May to August) appear to have conditions that match when the tornados occurr. This is a little concerning and I will need to look at some initial model fits to understand how they interact. 

The last major thing to think about is how sporadic these events occurr. Roughly 380 events in the area of interest after duplicates are removed in a period of 24 * 365 days. I am worried that this may leak into my train test splits and something may need to be done in order to rectify this. 

## Model Creation and Tuning

Because of the issue of a small sample side I mentioned previously, I am electing to do a 40/60 test/train split. I am doing this so that the datasets both have ample data and one set does not have only 15-20 points. I am trying to remedy this here and will adjust as needed. I got rid of the date section here as well as the time_of_year column encompasses this variance and is more apt to the seasonal nature of the tornado event. 
```{python}
#| label: pca
#| message: false
def DR_PCA(df):
    numerical_cols = df.select_dtypes(include = ['int64', 'float64']).columns

    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(df[numerical_cols])

    pca = PCA(n_components = 0.95)

    reduced_data = pca.fit_transform(scaled_data)

    loading_scores = pd.DataFrame(pca.components_, columns = numerical_cols)

    pca_full = PCA()
    pca_full.fit(scaled_data)
    cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)

    plt.figure(figsize = (8, 6))
    plt.plot(cumulative_variance)
    plt.xlabel('Number of Components')
    plt.ylabel('Cumulative Explained Variance')
    plt.title('Scree Plot')
    plt.grid(True)

    inflection_point = np.argmax(cumulative_variance >= 0.95) + 1  
    plt.axvline(x=inflection_point, color='red', linestyle='--')
    plt.axhline(y=cumulative_variance[inflection_point], color='red', linestyle='--')
    plt.text(inflection_point+1, 0.5, f'Inflection Point:\n{inflection_point} Components', color = 'red')

    plt.show()

    return loading_scores

def get_top_features_for_each_component(loading_scores, threshold):
    top_features = {}
    # print(loading_scores.shape[0])
    for i in range(loading_scores.shape[0]):
        component = f"Component {i+1}"
        scores = loading_scores.iloc[i]
        top_features_for_component = scores[abs(scores) > threshold].index.tolist()
        top_features[component] = top_features_for_component
    return top_features

pca_data = DR_PCA(AllDaysWithTornados)
top_feats = get_top_features_for_each_component(pca_data, 0.7)
print(top_feats)
pca_data.head()
```

```{python}
#| label: tt-split
#| message: false

ScaledTornados = AllDaysWithTornados.copy()
ScaledTornados.head()
dropped = ScaledTornados.drop('tornado_occurred', axis = 1)
numerical_cols = dropped.select_dtypes(include = ['int64', 'float64']).columns
scaler = StandardScaler()
ScaledTornados[numerical_cols] = scaler.fit_transform(ScaledTornados[numerical_cols])

feature_names = ScaledTornados.drop(['tornado_occurred', 'CZ_NAME_STR', 'TOR_F_SCALE', 'TOR_LENGTH', 'TOR_WIDTH', 'date'], axis = 1).columns.to_list()
X = ScaledTornados[feature_names]
y = ScaledTornados['tornado_occurred']

X_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.2, random_state=63)

X_train_with_const = sm.add_constant(X_train)
X_test_with_const = sm.add_constant(X_test)
X_train_with_const = X_train_with_const.astype(int)
X_test_with_const = X_test_with_const.astype(int)

unique, counts = np.unique(y_train, return_counts=True)
for val, count in zip(unique, counts):
    print(f"Value Train {val}: Count Train {count}")

unique, counts = np.unique(y_test, return_counts=True)
for val, count in zip(unique, counts):
    print(f"Value Test {val}: Count Test {count}")
```

# Regression Models

For this analysis I chose to create and tune 4 different classification models. All undergoing random search cross validation over a generated parameter sweep. For  each model I deemed f1 score to be the best measure of fit because it balances percision and recall. This allows some models to fully predict 

```{python}
#| label: logreg-model
#| message: false
# Model Creation
# All param grids generated by copilot when asked for a param grid for {model_type}

logreg_params = {
    'C': [0.01, 0.1, 1, 10, 100],
    'penalty': ['l2', 'l1'],
    'solver': ['liblinear', 'saga'],
    'class_weight': [None, 'balanced'],
    'max_iter': [100, 500, 1000]
}

logreg_model = LogisticRegression(random_state=63)
logreg_grid = RandomizedSearchCV(logreg_model, param_distributions = logreg_params, cv = 5, scoring = 'recall_macro', n_iter = 100, random_state = 63)
logreg_grid.fit(X_train, y_train)
```

* **Logistic Regression**
  * Rationale: 
  * Chosen Hyper-Parameters:
  ```{python}
  print("Best LR parameters:", logreg_grid.best_params_)
  ```
  * Results: 
  ```{python}
  print("Best LR score:", logreg_grid.best_score_)
  ```


```{python}
#| label: bayes-model
#| message: false
# Model Creation
# All param grids generated by copilot when asked for a param grid for {model_type}
bayes_params = {
    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5, 1e-4]
}

bayes_model = GaussianNB()
bayes_grid = RandomizedSearchCV(bayes_model, param_distributions = bayes_params, cv = 5, scoring = 'recall_macro', n_iter = 100, random_state = 63)
bayes_grid.fit(X_train, y_train)
```

bayes_grid.fit(X_train, y_train)
* **Naive Bayes**
  * Rationale: 
  * Chosen Hyper-Parameters:
    ```{python}
    print("Best Bayes parameters:", bayes_grid.best_params_)
    ```
  * Results: 
   ```{python}
   print("Best Bayes score:", bayes_grid.best_score_)
   ```
```{python}
#| label: svc-model
#| message: false
# Model Creation
# All param grids generated by copilot when asked for a param grid for {model_type}
svc_params = {
    'C': [0.1, 1, 10, 100],
    'kernel': ['linear', 'rbf', 'poly'],
    'gamma': ['scale', 'auto', 0.01, 0.1, 1],
    'degree': [2, 3, 4]  # Only used for 'poly' kernel
}
svc_model = svm.SVC(random_state = 63)
svc_grid = RandomizedSearchCV(svc_model, param_distributions = svc_params, cv = 5, scoring = 'recall_macro', n_iter = 100, random_state = 63)
svc_grid.fit(X_train, y_train)
```

* **Vector Control Regression**
  * Rationale: 
  * Chosen Hyper-Parameters:
    ```{python}
    print("Best SVC parameters:", svc_grid.best_params_)
    ```
  * Results: 
   ```{python}
   print("Best SVC score:", svc_grid.best_score_)
   ```
```{python}
#| label: rf-model
#| message: false
# Model Creation
# All param grids generated by copilot when asked for a param grid for {model_type}


rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [None, 5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['auto', 'sqrt', 'log2']
}
rf_model = RandomForestClassifier(random_state = 63)
rf_grid = RandomizedSearchCV(rf_model, rf_params, cv = 5, scoring = 'recall_macro', n_iter = 100, random_state = 63)
rf_grid.fit(X_train, y_train)
```
* **Random Forest Classifier**
  * Rationale: 
  * Chosen Hyper-Parameters:
    ```{python}
    print("Best RF parameters:", rf_grid.best_params_)
    ```
  * Results: 
   ```{python}
   print("Best RF score:", rf_grid.best_score_)
   ```


```{python}
#| label: rf-model
#| message: false
# Model Creation
# All param grids generated by copilot when asked for a param grid for {model_type}

best_logreg_model = logreg_grid.best_estimator_
best_bayes_model = bayes_grid.best_estimator_
best_svc_model = svc_grid.best_estimator_
best_rf_model = rf_grid.best_estimator_

```

## Results and Analysis
```{python}
def show_metrics(model, X_test, y_test):

    y_pred = model.predict(X_test)
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    cm = confusion_matrix(y_test, y_pred)
    ConfusionMatrixDisplay(cm).plot()

    # y_prob = model.predict_proba(X_test)[:, 1]
    # fpr, tpr, _ = roc_curve(y_test, y_prob)
    # plt.plot(fpr, tpr)
    # plt.xlabel("False Positive Rate")
    # plt.ylabel("True Positive Rate")
    # plt.title("ROC Curve")
    # plt.show()
    # print("AUC:", roc_auc_score(y_test, y_prob))
    
show_metrics(best_logreg_model, X_test, y_test)
show_metrics(best_bayes_model, X_test, y_test)
show_metrics(best_svc_model, X_test, y_test)
show_metrics(best_rf_model, X_test, y_test)

#Determine which model is best 
best_model = best_logreg_model
    
```
TODO: Add Results here.

TODO: Add Analysis here.

## App/Dashboard
```{python}
#| label: app
#| message: false
# Configure and run App

# class TornadoPredictorApp(tk.Tk):
#     def __init__(self, model, feature_names):
#         super().__init__()
#         self.title("Tornado Prediction Based of Daily Wather")
#         self.model = model
#         self.feature_names = feature_names
#         self.inputs = {}
#         self.create_widgets()
#         self.plot_prediction()

#     def create_widgets(self):
#         input_frame = ttk.Frame(self)
#         input_frame.pack(side=tk.LEFT, fill=tk.Y, padx=10, pady=10)

#         for i, name in enumerate(self.feature_names):
#             label = ttk.Label(input_frame, text=name)
#             label.grid(row=i, column=0, sticky=tk.W)
#             slider = ttk.Scale(input_frame, from_=0, to=100, orient=tk.HORIZONTAL, command=self.on_change)
#             slider.set(50)
#             slider.grid(row=i, column=1, padx=5, pady=2)
#             self.inputs[name] = slider

#         self.plot_frame = ttk.Frame(self)
#         self.plot_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)

#     def get_input_values(self):
#         return np.array([self.inputs[name].get() for name in self.feature_names]).reshape(1, -1)

#     def plot_prediction(self):
#         for widget in self.plot_frame.winfo_children():
#             widget.destroy()
#         values = self.get_input_values()
#         pred = self.model.predict(values)[0]

#         fig, ax = plt.subplots(figsize=(4, 3))
#         ax.bar(['Tornado Probability'], [pred], color='red')
#         ax.set_ylim(0, 1)
#         ax.set_ylabel('Predicted Probability')
#         ax.set_title('Tornado Prediction')

#         canvas = FigureCanvasTkAgg(fig, master=self.plot_frame)
#         canvas.draw()
#         canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)

#     def on_change(self, event):
#         self.plot_prediction()
    
    
# app = TornadoPredictorApp(model, feature_names)
# app.mainloop()

print("HI")

```
TODO: Add App/Dashboard here.

